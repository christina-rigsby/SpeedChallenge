{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Speed Challenge Framework",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christina-rigsby/SpeedChallenge/blob/Similarity/Copy_of_Speed_Challenge_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTMpzey5z5c",
        "colab_type": "text"
      },
      "source": [
        "**Comma AI Speed Challenge**\n",
        "\n",
        "  This notebook will contain (hopefully) all of the functions you need to import the data into your model.\n",
        "\n",
        "  ***Be sure to train with GPU acceleration enabled***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1abf-e_pFNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f9a8bb22-9dde-4fe5-86db-072c3293f179"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kq-rlJxzHbr",
        "colab_type": "text"
      },
      "source": [
        "**Import Statements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sguc70jY5xt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, Lambda, Dot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNzSFHCVDKGp",
        "colab_type": "text"
      },
      "source": [
        "**Custom Data Generator**\n",
        "\n",
        "This works (I think) for giving two sequential images to a Keras Functional model as well as the velocity associated with the second image.\n",
        "\n",
        "At this point, don't worry about how this works. If you need something changed or fixed, just ask. This is the boring part anyways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXXe8EiADaTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.direct = \"./drive/My Drive/commai_dataset/\"\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        \n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = self.list_IDs[index]\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.load(self.direct+\"data/data_\" + list_IDs_temp +\".npy\")\n",
        "        x1 = X[0:101,:,:,:]\n",
        "        x2 = X[1:102,:,:,:]\n",
        "        y = np.load(self.direct+\"labels/label_\" + list_IDs_temp +\".npy\")\n",
        "        y = y[1:]\n",
        "\n",
        "        return [x1, x2], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJy-HdjtEH-o",
        "colab_type": "text"
      },
      "source": [
        "**Define custom loss function**\n",
        "\n",
        "This is not well tested, neither is it optimized. You might not even want to use this function.\n",
        "\n",
        "Keras backend functions are a powerful tool for writing custom loss functions. To define a loss function it just has to accept *y_true* and *y_pred* as arguments and return a float.\n",
        "\n",
        "To use your new loss function, change the argument in *model.compile()*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqNfdCNENGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_sq_err(y_true, y_pred):\n",
        "    return tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgc5HQbfFvN1",
        "colab_type": "text"
      },
      "source": [
        "**Define the test-train split and create the Data Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru6ugzA6Fzc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'dim': (110,320),\n",
        "          'batch_size': 101,\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 3,\n",
        "          'shuffle': False}\n",
        "\n",
        "train_data = []\n",
        "train_label = []\n",
        "valid_data = []\n",
        "valid_label = []\n",
        "\n",
        "for i in range(70):\n",
        "    train_data.append(\"%03d\" %i)\n",
        "    train_data.append(\"%03d\" %(i+100))\n",
        "\n",
        "for i in range(70, 100):\n",
        "    valid_data.append(\"%03d\" %i)\n",
        "    valid_data.append(\"%03d\" %(i+100))\n",
        "\n",
        "partition={'train':train_data, 'validation':valid_data}\n",
        "labels = {'train': train_label,'validation':valid_label}\n",
        "\n",
        "training_generator = DataGenerator(partition['train'], labels['train'], **params)\n",
        "validation_generator = DataGenerator(partition['validation'], labels['validation'], **params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0xuO74GB9N",
        "colab_type": "text"
      },
      "source": [
        "**Define the input layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BszGQ-CEGEDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_A = tf.keras.layers.Input(shape=(110, 320, 3), name=\"first_image\")\n",
        "input_B = tf.keras.layers.Input(shape=(110, 320, 3), name=\"second_image\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijPPfw9oUmBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d414aab2-d929-4118-ddfa-9f7318f5a64f"
      },
      "source": [
        "print(input_A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"first_image:0\", shape=(None, 110, 320, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfgLVnl4GHC1",
        "colab_type": "text"
      },
      "source": [
        "**Define the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pHQxu2NGQ_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sequential model won't work with two inputs\n",
        "#model = keras.Sequential()\n",
        "\n",
        "#model.add(Conv2D(64, (10,10), activation='relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "#model.add(Conv2D(128, (7,7), activation='relu'))\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(1000, activation='sigmoid'))\n",
        "#model = tf.keras.models.Model(inputs=[input_A, input_B], outputs=[out])\n",
        "\n",
        "#Define model by specifying its forward pass:\n",
        "def MyModel(inp):\n",
        "  conv1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
        "  maxpool1 = MaxPooling2D()(conv1)\n",
        "  conv2 = Conv2D(128, (7,7), activation = 'relu')(maxpool1)\n",
        "  maxpool2 = MaxPooling2D()(conv2)\n",
        "  conv3 = Conv2D(128, (4,4))(maxpool2)\n",
        "  maxpool3 = MaxPooling2D()(conv3)\n",
        "  conv4 = Conv2D(256, (4,4), activation='relu')(maxpool3)\n",
        "  flat = Flatten()(conv4)\n",
        "  out = Dense(4096, activation='sigmoid')(flat)\n",
        "  return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDciexSEGPl0",
        "colab_type": "text"
      },
      "source": [
        "Generate the feature vectors (encodings) for both images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR3Bcp3iG4WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding_A = MyModel(input_A)\n",
        "encoding_B = MyModel(input_B) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kei_EEKcHX2H",
        "colab_type": "text"
      },
      "source": [
        "Add layer to compute absolute difference between encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuSrfRN3Iw0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L1_layer = Lambda(lambda tensors:tf.keras.backend.abs(tensors[0]-tensors[1]))\n",
        "L1_distance = L1_layer([encoding_A, encoding_B])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG5kDmL9IyGc",
        "colab_type": "text"
      },
      "source": [
        "Add dense layer to generate the similarity score between images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZxvRRuBRAcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similarity = Dense(1, activation='sigmoid')(L1_distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfsTNb0SRRqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siamese_net = tf.keras.models.Model(inputs=[input_A, input_B], outputs=similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fSlCiE_GUit",
        "colab_type": "text"
      },
      "source": [
        "**Model Details**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT1HSaxaGanK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "4f99bff3-8b70-4c66-814a-ed2c8fdb7a4e"
      },
      "source": [
        "siamese_net.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "first_image (InputLayer)        [(None, 110, 320, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "second_image (InputLayer)       [(None, 110, 320, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 101, 311, 64) 19264       first_image[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 101, 311, 64) 19264       second_image[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 50, 155, 64)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 50, 155, 64)  0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 44, 149, 128) 401536      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 44, 149, 128) 401536      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 22, 74, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 22, 74, 128)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 19, 71, 128)  262272      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 19, 71, 128)  262272      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 9, 35, 128)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 9, 35, 128)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 32, 256)   524544      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 6, 32, 256)   524544      max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 49152)        0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 49152)        0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4096)         201330688   flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4096)         201330688   flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 4096)         0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            4097        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 405,080,705\n",
            "Trainable params: 405,080,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKXx_R9GgXH",
        "colab_type": "text"
      },
      "source": [
        "**Declare the optimizer and loss function, then compile your *less ridiculous*  model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTDWZaewGtPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_optimizer = tf.keras.optimizers.Adam()\n",
        "siamese_net.compile(optimizer=_optimizer, loss = sum_sq_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zUadpkHe9j",
        "colab_type": "text"
      },
      "source": [
        "**Train using the fit_generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHc7Sk3IHiL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "b11904db-63ff-45fd-fcd4-bc2e42085fc9"
      },
      "source": [
        "siamese_net.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-59253182eceb>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/5\n",
            "140/140 [==============================] - 416s 3s/step - loss: 21411.1719 - val_loss: 14864.7969\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 435s 3s/step - loss: 21388.8359 - val_loss: 14864.7969\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 431s 3s/step - loss: 21388.8320 - val_loss: 14864.7969\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 431s 3s/step - loss: 21388.8281 - val_loss: 14864.7969\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 436s 3s/step - loss: 21388.8320 - val_loss: 14864.7969\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa6f0584518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyAf6toVbkFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60f8e984-442f-4eb0-9005-5079822f5c1d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}